# -*- coding: utf-8 -*-
"""MD&A_Modficiation_Score.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yQHELwt-ge_zwDmW83AneFG0fttS7Mmh
"""

import numpy as np
import os

import re
import pandas as pd
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

nltk.download('stopwords')

nltk.download('punkt')

stopword = set(stopwords.words('english'))

pip install pdfminer.six

from pdfminer.high_level import extract_text

from google.colab import drive
drive.mount('/content/drive/')

def mda_text_extractor(file_name,start_page, end_page):
  text = extract_text(file_name,page_numbers=list(np.arange(start_page,end_page+1)))
  return text

def write_to_text(textf, name):
  os.makedirs(os.path.dirname(f'MD&A_text_files/{name}'), exist_ok=True)
  with open(f'MD&A_text_files/{name}', 'w',encoding='utf-8') as ofile:
    ofile.write(textf)

def prepare(name):  
  mylines = []                            
  with open (f'MD&A_text_files/{name}', 'rt',encoding='utf-8', errors='ignore') as myfile:
      for line in myfile:
          line1 = re.sub('[^a-zA-Z]',' ', line) #removing all except words
          line2 = line1.rstrip().lstrip().lower() 
          mylines.append(line2) 
  return mylines

def tokenizing(lines,stopword):
  corpus = " ".join(lines)
  corpus_tokenized = nltk.word_tokenize(corpus)
  corpus_tokenized_s = [word for word in corpus_tokenized if not word in stopword]
  corpus_cleaned = " ".join(corpus_tokenized_s)
  corpus_cleaned =np.array(corpus_cleaned).reshape(-1)

  return corpus_cleaned  


def tfidf_vsm(corpus_cleaned):
  vectorizer = TfidfVectorizer()
  x =  vectorizer.fit_transform(corpus_cleaned).toarray()
  x_df = pd.DataFrame(x)

  return x, x_df

def convert_to_vsm(file_name, start_page, end_page, out_name):
  text = mda_text_extractor(file_name,start_page, end_page)
  write_to_text(text,out_name)
  lines = prepare(out_name)
  corpus_cleaned = tokenizing(lines,stopword)
  x , x_df = tfidf_vsm(corpus_cleaned)

  return x, x_df


def calculate_score(x,y):

  return sum([i*j for i,j in zip(x[0],y[0])])

def calculate_modification_score(doc1, start_page1, end_page1, out_name1,doc2, start_page2, end_page2, out_name2):
  x, x_df = convert_to_vsm(doc1, start_page1, end_page1, out_name1)
  y, y_df = convert_to_vsm(doc2, start_page2, end_page2, out_name2)
  return calculate_score(x,y)

doc1 = 'Air_Canada_-_Form_Annual_Report(Mar-29-2018).pdf' 
start_page1 = 2
end_page1 = 78
out_name1 = 'Air_Canada_(Mar-29-2018).txt'
doc2 = 'Air_Canada_-_Form_Annual_Report(Mar-29-2019).pdf' 
start_page2 = 2
end_page2 = 88
out_name2 = 'Air_Canada_(Mar-29-2019).txt'

calculate_modification_score(doc1, start_page1, end_page1, out_name1,doc2, start_page2, end_page2, out_name2)









